Part1
---

gbdt模型可以认为是k个基模型组成的一个加法运算式：  
$$\hat{y}_i=\sum_{k=1}^Kf_k(x), f_k\in F$$  
其中F指所有基模型组成的函数空间。   
那么一般化损失函数，是预测值$\hat{y}$和真实值$y$的关系。对于n个样本来说，可以写成：  
$$L=\sum_{i=1}^nl(y_i, \hat{y}_i)$$  
更一般的，一个好的模型在偏差和方差上有一个比较好的平衡，而损失函数代表了模型的偏差面，最小化损失函数就相当于最小化模型的偏差，但同时我们也需要兼顾模型的方差(因为我们不希望去适应噪声，而丰富模型有更高的方差，这使得模型很坏并且泛化界限受到惩罚)，所以目标函数还需要有抑制模型复杂度的正则项，因此目标函数可以写成    
$$Obj=\sum_{i=1}^nl(y_i, \hat{y}_i)+\sum_{k=1}^{K}\Omega(f_k)$$  
其中$\Omega$代表了基模型的复杂度，如果基模型是树模型，则树深度、叶子节点数等指标可以反映数的复杂程度。  

在加法模型的第t步，模型对第i个样本$x_i$的预测为：  
$$\hat{y}_i^t=\hat{y}_i^{t-1}+f_t(x_i)$$  
其中$f_t$就是这次需要加入的新模型，即需要拟合的模型，此时目标函数就可以写成：  
$$Obj^{(t)}=\sum_{i=1}^nl(y_i, \hat{y}_i^t)+\sum_{i=1}^t\Omega(f_i)$$  
$$=\sum_{i=1}^nl(y_i,\hat{y}_i^{t-1}+f_t(x_i))+\Omega(f_t)+C\quad(1)$$  
C代表常数，此时最优化目标函数，就相当于求得了$f_t$  

---------

Part2
---
根据泰勒展开式：$f(x+\Delta x)\approx f(x)+f'(x)\Delta x + \frac{1}{2}f''(x)\Delta x^2\quad(2)$  
等式(1)中，我们把$\hat{y}_i^{t-1}$看为x，$f_t(x_i)$看为$\Delta x$,那么等式(1)可以写成:  
$$Obj^{(t)}=\sum_{i=1}^t[l(y_i,\hat{y}_i^{t-1})+g_if_t(x_i)+\frac{1}{2}h_if_t^2(x_i)]+\Omega(f_t)+C\quad(3)$$  
其中$g_i$为损失函数一阶导，$h_i$为损失函数二阶导(注意这里是对$\hat{y}_i^{t-1}$求导)。由于在第t步，$\hat{y}_i^{t-1}$是一个已知值，所以$l(y_i,\hat{y}_i^{t-1})$是一个常数，对函数优化没有影响，因此优化等式(3)等价于优化下式：   
$$Obj^{(t)}=\sum_{i=1}^n[g_if_t(x_i)+\frac{1}{2}h_if_t^2(x_i)]+\Omega(f_t) \quad(4)$$  
因此我们只要求出每一步损失函数的一阶和二阶导(由于前一步的$\hat{y}^{t-1}$是已知的，所以这两个值就是常数), 代入等式4，然后最优化目标函数，就可以得到每一步的$f(x)$，最后根据加法模型得到一个整体模型。比如损失函数是平方损失函数时，$g_i=2(\hat{y}_i^{t-1}-y_i),h_i=2$  

----

Part3
---
到这里我们还没有假设基模型的形式，如果假设boosting的基模型用决策树来实现，则一颗生成好的决策树，即结构确定，也就是说树的叶子结点其实是确定了的。假设这棵树的叶子结点有T片叶子，而每片叶子对应的值$\omega\in R^T$。则存在一函数$q:R^d\rightarrow\{1,2, ... , T\}$,即将每个样本映射到某一个叶子节点上。那么这里$f_t(x)$可以换个写法，写成$\omega_{q(x)}$，这里q(x)代表了每个样本在哪个叶子结点上，而$\omega_q$表示了哪个叶子结点取什么$\omega$值，所以$\omega_{q(x)}$就代表了每个样本的取值$\omega$（即预测值）。  
决策树复杂度，可以由正则项定义：  
$$\Omega(f_t)=\gamma T+\frac{1}{2}\lambda\sum_{j=1}^T\omega_j^2$$  
即决策树模型的复杂度由生成的树的叶子节点数量和叶子节点对应的值向量的L2范数决定。  
假设$I_j=\{i|q(x_i)=j\}$为第j个叶子节点的样本集合，那么等式4可以改写为：  
$$Obj^{(t)}=\sum_{i=1}^n[g_if_t(x_i)+\frac{1}{2}h_if_t^2(x_i)]+\Omega(f_t)$$  
$$=\sum_{i=1}^n[g_i\omega_{q(x_i)}+\frac{1}{2}h_i\omega_{q(x_i)}^2]+\gamma T+\frac{1}{2}\lambda\sum_{j=1}^T\omega_j^2$$  
$$=\sum_{j=1}^{T}[(\sum_{i\in I_j}g_i)\omega_j+\frac{1}{2}(\sum_{i\in I_j}h_i+\lambda)\omega_j^2]+\gamma T$$  
$$=\sum_{j=1}^T[G_j\omega_j+\frac{1}{2}(H_j+\lambda)\omega_j^2]+\gamma T \quad (5)$$  
其中$G_j=\sum_{i\in I_j}g_i, H_j=\sum_{i\in I_j}h_i$  
如果树时确定的，即q是确定的，或者说我们已经知道了每个叶子结点有哪些样本，所以$G_j$和$H_j$是确定的，但$\omega$是不确定的($\omega$就是需要预测的值)，那么令一阶导数为0，就可以求出叶子节点j对应的值：  
$$\omega_j^*=-\frac{G_j}{H_j+\lambda} \quad (6)$$  
因此目标函数可以化简为：  
$$Obj^{(t)}=-\frac{1}{2}\sum_{j=1}^T\frac{G_j^2}{H_j+\lambda}+\gamma T \quad (7)$$  

----

Part4
---
如何最优化目标函数?  
那么对于单棵决策树，一种理想的优化状态就是枚举所有可能的树结构，因此过程如下：

a、首先枚举所有可能的树结构，即 q；

b、计算每种树结构下的目标函数值，即等式7的值；

c、取目标函数最小（大）值为最佳的数结构，根据等式6求得每个叶子节点的$\omega$取值，即样本的预测值。

但上面的方法肯定是不可行的，因为树的结构千千万，所以一般用贪心策略来优化：  
a、从深度为0的树开始，对每个叶节点枚举所有的可用特征

b、 针对每个特征，把属于该节点的训练样本根据该特征值升序排列，通过线性扫描的方式来决定该特征的最佳分裂点，并记录该特征的最大收益（采用最佳分裂点时的收益）

c、 选择收益最大的特征作为分裂特征，用该特征的最佳分裂点作为分裂位置，把该节点生长出左右两个新的叶节点，并为每个新节点关联对应的样本集

d、回到第1步，递归执行到满足特定条件为止。  

计算收益的方法(紧扣目标函数)：假设我们在某一节点上二分裂成两个节点，分别是左（L）右（R），则分裂前的目标函数是  
$$-\frac{1}{2}[\frac{(G_L+G_R)^2}{H_L+H_R+\lambda}]+\gamma$$  
分裂后是：   
$$-\frac{1}{2}[\frac{G_L^2}{H_L+\lambda}+\frac{G_R^2}{H_R+\lambda}]+2\gamma$$  
则对于目标函数来说，分裂后的收益是(这里假设是最小化目标函数，所以用分裂前-分裂后):    
$$Gain=\frac{1}{2}[\frac{G_L^2}{H_L+\lambda}+\frac{G_R^2}{H_R+\lambda}-\frac{(G_L+G_R)^2}{H_L+H_R+\lambda}]-\gamma \quad (8)$$  
上式计算出来的收益，也可以作为变量重要度输出的一个依据。  
所以xgboost算法总结为：  
a、算法在拟合的每一步都新生成一颗决策树；

b、在拟合这棵树之前，需要计算损失函数在每个样本上的一阶导和二阶导，即$g_i$和$h_i$；

c、通过上面的贪心策略生成一颗树，计算每个叶子结点的的$G_j$和$H_j$，利用等式6计算预测值$\omega$；

d、把新生成的决策树$f_t(x)$加入$\hat{y}_i^t=\hat{y}_i^{t-1}+\rho f_t(x_i)$，其中$\rho$为学习率。  

--------
参考资料：  
https://zhuanlan.zhihu.com/p/29765582