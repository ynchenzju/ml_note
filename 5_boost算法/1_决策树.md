1. 熵：表示随机变量不确定性的度量。假设X是一个取有限个值的离散随机变量，概率分布为  
   $$P(X=x_i)=p_i, i=1,2,...,n$$  
   则随机变量X的熵定义为    
   $$H(X)=-\sum_i^np_ilnp_i$$
   其中若$p_i=0$，定义$0ln0=0$。
2. 用特征A对训练数据集D进行划分得到的信息增益=经验熵-经验条件熵  
   $$g(D,A)=H(D)-H(D|A)$$
3. 使用信息增益作为划分训练数据集的特征，存在于偏向于选择取值较多的特征的问题。使用信息增益比对这个问题进行校正：特征A对训练数据集D的信息增益比$g_R(D,A)$定义为其信息增益比$g(D,A)$与训练数据集D关于特征A的值的熵$H_A(D)$之比，即  
   $$g_R(D,A)=\frac{D,A}{H_A(D)}$$  
   其中  
   $$H_A(D)=-\sum_{i=1}^n\frac{|D_i|}{D}ln\frac{|D_i|}{D}$$  
   n是特征A取值的个数

4. 决策树算法：ID3使用信息增益进行训练，C4.5使用信息增益比进行训练。具体可以例子可以看https://zhuanlan.zhihu.com/p/26760551  

-----

对于决策树算法中，一个特征能否重复使用(放回)的思考：  
---
首先描述一下C4.5算法：  
输入：训练数据集D，特征集A，阈值$\epsilon$  
输出：决策树T  
(1) 如果D中所有实例都属于同一类$C_k$, 则置T为单节点树，并将$C_k$作为该节点的类，返回T。  
(2) 如果$A=\emptyset$, 则置T为单节点树，并将D中实例数最大的类$C_k$作为该节点的类，返回T。  
(3) 否则，计算A中各特征对D的信息增益比，选择信息增益比最大的特征$A_g$。  
(4) 如果$A_g$的信息增益比小于阈值$\epsilon$，则置T为单节点树，并将D中实例数最大的类$C_k$作为该节点的类，返回T。  
(5) 否则，对$A_g$的<font color=red>所有</font>可能值$a_i$，按照$A_g=a_i$将D分割为子集若干非空$D_i$，将$D_i$中实例数最大的类作为标记，构建子节点，由节点和其子节点构成树T，返回T。  
(6) 对节点i，以$D_i$为训练集，以$A-\{A_g\}$为特征集，递归调用(1)-(5)，得到子树$T_i$，返回$T_i$  

注意该算法中每次是利用一个特征的“所有”可能值，对节点进行迭代划分。也就是说这里包含几个意思：  
(1) 该特征取值是离散的(或者说可以按照某种方式将连续值作离散化)。  
(2) 每次利用该特征分裂子节点时，是利用该特征的所有可能值生成多叉树。  
这样，一个特征就可以说是被利用完全了，后续该特征在子树生成上就没有价值了。因此，这个算法里后续是从$A-\{A_g\}$中取特征，特征不放回。  
因此如果特征不满足以上两个条件，也就是说，如果特征取值是连续的(无法枚举)，或者每次利用该特征分裂子节点时，只是利用该特征的部分可能值生成多叉树，那么后续利用该特征可能仍然有信息增益比上的收益，因此在这种情况下，该特征是可以放回的。  
或者说，不存在一个特征能否放回的问题，而是一个特征有没有使用完全的问题。